Resources:
  DataLakeRawDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Raw data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName]]
        Name: data_lake_raw

  DataLakeProcessedDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Processed data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeProcessedDataBucket]]
        Name: data_lake_processed

  DataLakeCuratedDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Curated data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeCuratedDataBucket]]
        Name: data_lake_curated

  RawTitanicTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref DataLakeRawDatabase
      TableInput: 
        Description: Table with Titanic dataset
        Name: titanic
        Owner: data-engineering
        Parameters:
          skip.header.line.count: 1
        StorageDescriptor:
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Location: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName, 'titanic']]
          SerdeInfo:
            Parameters:
              separatorChar: ','
              escapeChar: '\'
              quoteChar: '"'
            SerializationLibrary: org.apache.hadoop.hive.serde2.OpenCSVSerde
          Columns:
            - Name: PassengerId
              Type: string
            - Name: PcClass
              Type: string
            - Name: Name
              Type: string
            - Name: Sex
              Type: string
            - Name: Age
              Type: string
            - Name: SibSp
              Type: string
            - Name: Parch
              Type: string
            - Name: Ticket
              Type: string
            - Name: Fare
              Type: string
            - Name: Cabin
              Type: string
            - Name: Embarked
              Type: string

  RawAtomicEventsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Configuration: "{\"Version\": 1.0,\"CrawlerOutput\": {\"Partitions\": {\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"
      DatabaseName: !Ref DataLakeRawDatabase
      Description: Raw Atomic Events Crawler
      Name: raw-atomic-events-crawler
      Role: !Ref IamRoleDataLakeGlue
      Schedule: 
        ScheduleExpression: 'cron(0 12 * * ? *)'
      SchemaChangePolicy: 
        DeleteBehavior: DEPRECATE_IN_DATABASE
        UpdateBehavior: UPDATE_IN_DATABASE
      Targets: 
        S3Targets:
          - Path: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName, 'atomic_events']]

  IamRoleDataLakeGlue:
    Type: AWS::IAM::Role
    Properties:
      RoleName: iam-role-service-datalake-glue
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      ManagedPolicyArns:
        - Ref: IamAccessPolicyDataLakeGlue
      Path: /

  IamAccessPolicyDataLakeGlue:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Policy to provide Glue access to the data lake
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
            Resource:
              - !ImportValue data-lake-DataLakeRawDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeRawDataBucketArn, '*']]
              - !ImportValue data-lake-DataLakeProcessedDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeProcessedDataBucketArn, '*']]
              - !ImportValue data-lake-DataLakeCuratedDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeCuratedDataBucketArn, '*']]
          - Effect: Allow
            Action: 
              - cloudwatch:PutMetricData
              - glue:*
              - logs:CreateLogGroup
              - logs:CreateLogStream
            Resource: '*'
Resources:
  DataLakeRawDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Raw data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName]]
        Name: data_lake_raw

  DataLakeProcessedDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Processed data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeProcessedDataBucket]]
        Name: data_lake_processed

  DataLakeCuratedDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Description: Curated data
        LocationUri: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeCuratedDataBucket]]
        Name: data_lake_curated

  RawTitanicTable:
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref DataLakeRawDatabase
      TableInput: 
        Description: Table with Titanic dataset
        Name: titanic
        Owner: data-engineering
        Parameters:
          skip.header.line.count: 1
        StorageDescriptor:
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          Location: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName, 'titanic']]
          SerdeInfo:
            Parameters:
              separatorChar: ','
              escapeChar: '\'
              quoteChar: '"'
            SerializationLibrary: org.apache.hadoop.hive.serde2.OpenCSVSerde
          Columns:
            - Name: PassengerId
              Type: string
            - Name: PcClass
              Type: string
            - Name: Name
              Type: string
            - Name: Sex
              Type: string
            - Name: Age
              Type: string
            - Name: SibSp
              Type: string
            - Name: Parch
              Type: string
            - Name: Ticket
              Type: string
            - Name: Fare
              Type: string
            - Name: Cabin
              Type: string
            - Name: Embarked
              Type: string

  RawAtomicEventsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Configuration: "{\"Version\": 1.0,\"CrawlerOutput\": {\"Partitions\": {\"AddOrUpdateBehavior\":\"InheritFromTable\"}}}"
      DatabaseName: !Ref DataLakeRawDatabase
      Description: Raw Atomic Events Crawler
      Name: raw-atomic-events-crawler
      Role: !Ref IamRoleDataLakeGlue
      Schedule: 
        ScheduleExpression: 'cron(0 12 * * ? *)'
      SchemaChangePolicy: 
        DeleteBehavior: DEPRECATE_IN_DATABASE
        UpdateBehavior: UPDATE_IN_DATABASE
      Targets: 
        S3Targets:
          - Path: !Join ['/', ['s3:/', !ImportValue data-lake-DataLakeRawDataBucketName, 'atomic_events']]

  IamRoleDataLakeGlue:
    Type: AWS::IAM::Role
    Properties:
      RoleName: iam-role-service-datalake-glue
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      ManagedPolicyArns:
        - Ref: IamAccessPolicyDataLakeGlue
      Path: /

  IamAccessPolicyDataLakeGlue:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      Description: Policy to provide Glue access to the data lake
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - s3:ListBucket
              - s3:GetObject
              - s3:PutObject
            Resource:
              - !ImportValue data-lake-DataLakeRawDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeRawDataBucketArn, '*']]
              - !ImportValue data-lake-DataLakeProcessedDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeProcessedDataBucketArn, '*']]
              - !ImportValue data-lake-DataLakeCuratedDataBucketArn
              - !Join ['/', [!ImportValue data-lake-DataLakeCuratedDataBucketArn, '*']]
          - Effect: Allow
            Action: 
              - cloudwatch:PutMetricData
              - glue:*
              - logs:CreateLogGroup
              - logs:CreateLogStream
            Resource: '*'
